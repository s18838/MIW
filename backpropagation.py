#!/usr/bin/env python3# -*- coding: utf-8 -*-"""Created on Sat Mar 27 17:10:02 2021@author: taraskulyavets"""import pandas as pdimport numpy as npimport argparseclass NeuralLayer:        def __init__(self, weights, bias, use_bipolar = False):        self.weights = weights        self.bias = bias        self.use_bipolar = use_bipolar            def unipolar_sigmoid(self, u_prim):        return 1 / (1 + np.exp(-u_prim))        def bipolar_sigmoid(self, u_prim):        return 2 / (1 + np.exp(-u_prim)) - 1        def activation_function(self, u_prim):        if self.use_bipolar:            return self.bipolar_sigmoid(u_prim)        else:            return self.unipolar_sigmoid(u_prim)        def feed_forward(self, x):        u = np.matmul(self.weights, x)        u_prim = u + self.bias        y = self.activation_function(u_prim)        self.outputs = y        return y        @staticmethod    def generate_random_layer(input_size, layer_size, bipolar):        weights = np.random.rand(layer_size, input_size) * 2 - 1         bias = np.random.rand(layer_size) * (-1)        return NeuralLayer(weights, bias, bipolar)     class NeuralNetwork:    def __init__(self, layers, learning_factor = 0.1, use_bipolar = False):        self.layers = layers        self.learning_factor = learning_factor        self.use_bipolar = use_bipolar            def derivative_unipolar_sigmoid(self, output):        return output * (1 - output)            def derivative_bipolar_sigmoid(self, output):        return 1 - output ** 2        def derivative_activation_function(self, output):        if self.use_bipolar:            return self.derivative_bipolar_sigmoid(output)        else:            return self.derivative_unipolar_sigmoid(output)            def feed_forward(self, inputs):        for layer in self.layers:            inputs = layer.feed_forward(inputs)        return inputs        def feed_backward(self, expected, row):        for i in reversed(range(len(self.layers))):            layer = self.layers[i]            errors = list()            if i != (len(self.layers) - 1):                errors =  np.matmul(self.layers[i + 1].delta, self.layers[i + 1].weights)                #for j in range(len(layer.weights)):                    #error = 0.0                    #for k in range(len(self.layers[i + 1].weights)):                    #    error += (self.layers[i + 1].weights[k][j] * self.layers[i + 1].delta[k])                    #errors.append(error)            else:                errors = expected - layer.outputs                #for j in range(len(layer.outputs)):                #    errors.append(expected[j] - layer.outputs[j])            layer.delta = errors * self.derivative_activation_function(layer.outputs)            #for j in range(len(layer.outputs)):                #layer.delta[j] = errors[j] * self.derivative_activation_function(layer.outputs[j])                            if i == 0:                inputs = row            else:                inputs = self.layers[i - 1].outputs            layer.weights += np.atleast_2d(layer.delta).T * inputs * self.learning_factor            layer.bias += layer.delta * self.learning_factor            #for index, neuron in enumerate(layer.weights):            #    neuron += layer.delta[index] * inputs * self.learning_factor            #    #for j in range(len(inputs)):            #    #    neuron[j] += layer.delta[index] * inputs[j] * self.learning_factor            #    layer.bias[index] += layer.delta[index] * self.learning_factor        def train(self, train, out):        sum_error = 0        for _ in range(100):            for row in train:                inputs = row[:-1].astype(np.float64)                outputs = self.feed_forward(inputs)                expected = [0 for i in range(len(out))]                expected[out.index(row[-1])] = 1                sum_r = sum([(expected[i]-outputs[i])**2 for i in range(len(expected))])                sum_error += sum_r                self.feed_backward(expected, inputs)            @staticmethod    def create_network(input_size, layers_sizes, learning_factor, bipolar):        layers = []        for layer_size in layers_sizes:            layers.append(NeuralLayer.generate_random_layer(input_size, layer_size, bipolar))            input_size = layer_size        return NeuralNetwork(layers, learning_factor, bipolar)   def test_network(file_name, test_split, learning_factor, hidden_size, bipolar):    layers_sizes = [hidden_size]        dataset = pd.read_csv(file_name)    dataset = dataset.iloc[:,1:]    train = dataset.sample(frac = (1 - test_split))    test = dataset.drop(train.index)    train = np.array(train)    test = test.to_numpy()        input_size = len(train[0]) - 1    out = list(set([row[-1] for row in np.array(dataset)]))    output_size = len(out)    layers_sizes.append(output_size)        network = NeuralNetwork.create_network(input_size, layers_sizes, learning_factor, bipolar)        network.train(train, out)        for row in test:        predict = out[network.feed_forward(row[:-1].astype(np.float64)).argmax()]        print(f"{row[-1]} {predict}")        def main(args):   test_network(args.input, args.test_split, args.learning_factor, args.hidden, args.bipolar)    def parse_arguments():    parser = argparse.ArgumentParser(description=("Backpropagation"))    parser.add_argument("-i", "--input",            action="store",            default="Iris.csv",            help="Input file for training network")    parser.add_argument("--test_split",            action="store",            type=float,            default=0.3,            help="What part of data should be used for validation (default 0.3)")    parser.add_argument("-l", "--learning_factor",            action="store",            help="Learning factor",             default=0.1)    parser.add_argument("--bipolar",            action="store_true",            help="If set use bipolar function otherwise unipolar")    parser.add_argument("-e", "--hidden",            action="store",            type=int,            help="Size of hidden layer",            default=4)    return parser.parse_args()if __name__ == '__main__':    main(parse_arguments())