#!/usr/bin/env python3# -*- coding: utf-8 -*-"""Created on Sat Mar 27 17:10:02 2021@author: taraskulyavets"""import pandas as pdimport numpy as npimport argparseclass NeuralLayer:        def __init__(self, weights, bias, bipolar = False):        self.weights = weights        self.bias = bias        self.delta = [None] * len(weights)        self.bipolar = bipolar            def sigmoid(self, u_prim):        return 1 / (1 + np.exp(-u_prim))        def bipolar_sigmoid(self, u_prim):        return (np.exp(u_prim) - np.exp(-u_prim)) / (np.exp(u_prim) + np.exp(-u_prim))        def activation_function(self, u_prim):        if self.bipolar:            return self.bipolar_sigmoid(u_prim)        else:            return self.sigmoid(u_prim)        def feed_forward(self, x):        u = np.matmul(self.weights, x)        u_prim = u + self.bias        y = self.activation_function(u_prim)        self.outputs = y        return y        @staticmethod    def generate_random_layer(input_size, layer_size, bipolar):        weights = np.random.rand(layer_size, input_size) * 2 - 1 # [-1, 1]        bias = np.random.rand(layer_size) * (-1) # [-1, 0]        return NeuralLayer(weights, bias, bipolar)     class NeuralNetwork:    def __init__(self, layers, learning_factor = 0.1, bipolar = False):        self.layers = layers        self.learning_factor = learning_factor        self.bipolar = bipolar            def derivative_sigmoid(self, output):        return output * (1 - output)            def derivative_bipolar_sigmoid(self, output):        return 1 - output ** 2        def derivative_activation_function(self, output):        if self.bipolar:            return self.derivative_bipolar_sigmoid(output)        else:            return self.derivative_sigmoid(output)            def feed_forward(self, inputs):        for layer in self.layers:            inputs = layer.feed_forward(inputs)        return inputs        def feed_backward(self, expected, row):        for i in reversed(range(len(self.layers))):            layer = self.layers[i]            errors = list()            if i != (len(self.layers) - 1):                for j in range(len(layer.weights)):                    error = 0.0                    for k in range(len(self.layers[i + 1].weights)):                        error += (self.layers[i + 1].weights[k][j] * self.layers[i + 1].delta[k])                    errors.append(error)            else:                for j in range(len(layer.outputs)):                    errors.append(expected[j] - layer.outputs[j])            for j in range(len(layer.outputs)):                layer.delta[j] = errors[j] * self.derivative_activation_function(layer.outputs[j])                #TODO check if before            #continue            if i == 0:                inputs = row            else:                inputs = self.layers[i - 1].outputs            for index, neuron in enumerate(layer.weights):                for j in range(len(inputs)):                    neuron[j] += layer.delta[index] * inputs[j] * self.learning_factor                layer.bias[index] += layer.delta[index] * self.learning_factor                #TODO check if after    def update_weights(self, inputs):        for i in range(len(self.layers)):            if i != 0:                inputs = self.layers[i - 1].outputs            for index, neuron in enumerate(self.layers[i].weights):                for j in range(len(inputs)):                    neuron[j] += self.layers[i].delta[index] * inputs[j] * self.learning_factor                self.layers[i].bias[index] += self.layers[i].delta[index] * self.learning_factor        def train(self, train, out):        for _ in range(100):            sum_error = 0            for row in train:                inputs = row[:-1].astype(np.float64)                outputs = self.feed_forward(inputs)                expected = [0 for i in range(len(out))]                expected[out.index(row[-1])] = 1                sum_r = sum([(expected[i]-outputs[i])**2 for i in range(len(expected))])#TODO check response                #print(sum_r)                sum_error += sum_r                self.feed_backward(expected, inputs)#TODO                #self.update_weights(inputs)            #print(sum_error / len(train))            #print("=" * 50)            @staticmethod    def create_network(input_size, layers_sizes, learning_factor, bipolar):        layers = []        for layer_size in layers_sizes:            layers.append(NeuralLayer.generate_random_layer(input_size, layer_size, bipolar))            input_size = layer_size        return NeuralNetwork(layers, learning_factor, bipolar)   def test_network(file_name, test_split, learning_factor, hidden_size, bipolar):    layers_sizes = [hidden_size]        dataset = pd.read_csv(file_name)    dataset = dataset.iloc[:,1:]    train = dataset.sample(frac = (1 - test_split))    test = dataset.drop(train.index)    train = np.array(train)    test = test.to_numpy()        print(test)        input_size = len(train[0]) - 1    out = list(set([row[-1] for row in np.array(dataset)]))    output_size = len(out)    layers_sizes.append(output_size)        network = NeuralNetwork.create_network(input_size, layers_sizes, learning_factor, bipolar)        network.train(train, out)        for row in test:        predict = out[network.feed_forward(row[:-1].astype(np.float64)).argmax()]        print(f"{row[-1]} {predict}")        def main(args):   test_network(args.input, args.test_split, args.learning_factor, args.hidden, args.bipolar)    def parse_arguments():    parser = argparse.ArgumentParser(description=("Backpropagation"))    parser.add_argument("-i", "--input",            action="store",            default="Iris.csv",            help="Input file for training network")    parser.add_argument("--test_split",            action="store",            type=float,            default=0.3,            help="What part of data should be used for validation (default 0.3)")    parser.add_argument("-l", "--learning_factor",            action="store",            help="Learning factor",             default=0.1)    parser.add_argument("--bipolar",            action="store_true",            help="If set use bipolar function otherwise unipolar")    parser.add_argument("-e", "--hidden",            action="store",            type=int,            help="Size of hidden layer",            default=4)    return parser.parse_args()if __name__ == '__main__':    main(parse_arguments())